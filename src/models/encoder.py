import torch.nn as nnfrom .attention import MultiHeadAttention, PositionWiseFFNclass EncoderLayer(nn.Module):    def __init__(self, n_head, d_model, dropout):        super().__init__()        # multi_head attention layer        self.mha = MultiHeadAttention(n_head, d_model, dropout)        # position_wise_ffn        self.ffn = PositionWiseFFN(d_model, dropout)    def forward(self, xz, mask):        context = self.mha(xz, xz, xz, mask)        output = self.ffn(context)        return outputclass Encoder(nn.Module):    def __init__(self, n_head, d_model, dropout, n_layer):        super().__init__()        self.encoder_layers = nn.ModuleList([EncoderLayer(n_head, d_model, dropout) for _ in range(n_layer)])    def forward(self, xz, mask):        for encoder in self.encoder_layers:            xz = encoder(xz, mask)        return xz